<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="VLM, Agent, ARPG, Benchmark, Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VARP Agent</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <strong>Anonymous Author(s)</strong>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://varp-agent.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
               <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://varp-agent.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <!-- <span class="icon"> -->
                    <!-- <i class="fab fa-database"></i> -->
                  <!-- </span> -->
                  <span>Dataset</span>
                </a> 

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <div style="text-align: center;">
        <iframe width="840" height="472" 
        src="https://www.youtube.com/embed/S1OWlF2LALM?si=gRWC9DmMV6UthtWu" 
        title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
        clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    
      <h2 class="subtitle has-text-centered">
        We propose a VARP agent framework and choose the action role-playing game (ARPG) "Black Myth: Wukong" 
        as a research platform to investigate the capabilities of existing vision-language models (VLMs) in 
        scenarios that necessitate visual-only input and intricate action outputs.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recently, large language model (LLM)-based agents have made significant advances across various fields. 
            One of the most popular research areas involves applying these agents to video games. Traditionally, 
            these methods have relied on game APIs to access in-game environmental and action data. However, this 
            approach is limited by the availability of APIs and does not reflect how humans play games. With the advent 
            of vision language models (VLMs), agents now have enhanced visual understanding capabilities, enabling them 
            to interact with games using only visual inputs. Despite these advances, current approaches still face challenges 
            in action-oriented tasks, particularly in action role-playing games (ARPGs), where reinforcement learning methods 
            are prevalent but suffer from poor generalization and require extensive training. To address these limitations, 
            we select an ARPG, "Black Myth: Wukong", as a research platform to explore the capability boundaries of existing 
            VLMs in scenarios requiring visual-only input and complex action output. We define 12 tasks within the game, 
            with 75% focusing on combat, and incorporate several state-of-the-art VLMs into this benchmark. Additionally, 
            we will release a human operation dataset containing recorded gameplay videos and operation logs, including mouse 
            and keyboard actions. Moreover, we propose a novel VARP (Vision Action Role-Playing) agent framework, consisting 
            of an action planning system and a visual trajectory system. Our framework demonstrates the ability to perform 
            basic tasks and succeed in 90% of easy and medium-level combat scenarios. This research aims to provide new insights 
            and directions for applying multimodal agents in complex action game environments. 
            The code and datasets will be released.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    <!-- Proposed Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">VARP Agent Framework</h2>
        <div class="content has-text-justified">
          <img src="./static/videos/VARP_Agent.png" alt="VARP">
          <p>
            <br>
            We propose a novel framework named the VARP agent, which directly takes game screenshots as input. 
            Through inference by a group of Vision-Language Models (VLMs), it ultimately generates actions in 
            the form of Python code, which can directly operate the game character. Each action is a sequence 
            that consists of various combinations of atomic commands. These atomic commands include light attack, 
            dodge, heavy attack, restore health, and others. Meanwhile, the VARP agent maintains three libraries: 
            a situation library, an action library, and a human-guided library. These libraries can be retrieved 
            and updated to store intensive knowledge for self-learning and human guidance. Overall, the VARP agent 
            is divided into two systems: the action planning system and the human-guided trajectory system. 
            In the action library, "def new_func_a()" represents the new action generated by the action planning 
            system, while "def new_func_h()" represents the action generated by the human-guided trajectory system. 
            "def pre_func()" represents the predefined actions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Proposed Method. -->  
    <br>
    <br>

    <!-- Case Studies. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Case Studies</h2>
        <div class="content has-text-justified">
          <img src="./static/videos/case_studies.png" alt="case">
          <p>
            <br>
             Case studies of actions and corresponding game screenshots. The actions in the first and second rows are 
             predefined functions. The actions in the third row are generated by the human-guided trajectory system.
             The new actions in the fourth and fifth rows are summarized by SOAG after each combat interaction between 
             the player character and the enemy and stored in the action library.
          </p>
        </div>
      </div>
    </div>
    <!--/ Case Studies. -->  


</section>




<footer class="footer">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align:center">
                    This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
                  <p style="text-align:center">
                    Website source code based on the <a
                    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                  </p>

              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
